{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RI_ProjetoFinal.ipynb","provenance":[{"file_id":"0B-Lpu7Fd7oOiOXRMYkhSMHl2eFcyX1l4VVFMeGs0T2RzdW1J","timestamp":1575506135622}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"te631tJEXAsX","colab_type":"text"},"source":["## Bibliotecas"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mySrqJaMJkkm","colab":{}},"source":["#===============================================\n","# APIs necessárias para instalar no Colab\n","#===============================================\n","\n","#!pip install neuralcoref\n","#!pip install scikit-multilearn\n","#!pip install gensim\n","#!pip install spacy==2.1.0\n","#!python -m spacy download en\n","#!python -m spacy download en_core_web_lg"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"T6ouCucTKMDi","colab":{}},"source":["#===============================================\n","# Importando as bibliotecas\n","#===============================================\n","import xml.etree.ElementTree as ET\n","import os\n","import pandas as pd\n","import numpy as np\n","import pickle\n","import re\n","import matplotlib.pyplot as plt\n","import gensim\n","import spacy\n","import en_core_web_lg\n","import neuralcoref\n","\n","from seg.newline.segmenter import NewLineSegmenter\n","from spacy import displacy\n","from collections import Counter, defaultdict\n","from google.colab import files\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n","from sklearn.pipeline import Pipeline\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from skmultilearn.problem_transform import LabelPowerset\n","from wordcloud import WordCloud\n","from nltk.tokenize import sent_tokenize, word_tokenize \n","from gensim.parsing.preprocessing import remove_stopwords\n","from gensim.models import Word2Vec"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MfOtB4g6V_6F","colab_type":"text"},"source":["## Construção do modelo"]},{"cell_type":"code","metadata":{"id":"lzeOcBtZzEhj","colab_type":"code","colab":{}},"source":["# Carrega todos os arquios do projeto\n","arquivo = files.upload()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UjEQvzbfqf-T","colab_type":"code","colab":{}},"source":["# Carrega o modelo treinado de NLP para o idioma inglês\n","nlp = en_core_web_lg.load()\n","neuralcoref.add_to_pipe(nlp)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E8S0wXjteXvj","colab_type":"code","colab":{}},"source":["# Carrega os dados do Opinion Lexicon\n","neg_file = open(\"neg_words.txt\", encoding = \"ISO-8859-1\")\n","pos_file = open(\"pos_words.txt\", encoding = \"ISO-8859-1\")\n","neg = [line.strip() for line in neg_file.readlines()]\n","pos = [line.strip() for line in pos_file.readlines()]\n","opinion_words = neg + pos"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_Z2YOwV8K2JX","colab":{}},"source":["# Carrega o dataset\n","tree = ET.parse(\"Laptops_Train.xml\")\n","root = tree.getroot()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ny78a-7vRcvn","outputId":"f2727dfb-0292-445f-f53e-5acd101c9606","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1575517605910,"user_tz":180,"elapsed":1170,"user":{"displayName":"Thiago Batista","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCEVEmptKeAM5s4J7Gr7nNwWwrHzQd8tnIfJQ63Eg=s64","userId":"15252053828205320610"}}},"source":["# Carrega todos os termos que representam um aspecto encontrados nos reviews\n","labeled_reviews = []\n","for sentence in root.findall(\"sentence\"):\n","    entry = {}\n","    aspects = []\n","\n","    if sentence.find(\"aspectTerms\"):\n","        for aterm in sentence.find(\"aspectTerms\").findall(\"aspectTerm\"):\n","            aspects.append(aterm.get(\"term\"))\n","\n","    entry[\"text\"], entry[\"aspects\"] = sentence[0].text, aspects\n","    labeled_reviews.append(entry)\n","\n","labeled_df = pd.DataFrame(labeled_reviews)\n","print(\"Existem\", len(labeled_reviews), \"reviews no conjunto de treinamento.\")"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Existem 3048 reviews no conjunto de treinamento.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7mWQJHeneXwR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"95aa3dab-f479-40ee-e246-54b7645e761b","executionInfo":{"status":"ok","timestamp":1575517612491,"user_tz":180,"elapsed":1427,"user":{"displayName":"Thiago Batista","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCEVEmptKeAM5s4J7Gr7nNwWwrHzQd8tnIfJQ63Eg=s64","userId":"15252053828205320610"}}},"source":["# Lista todos os reviews que não possuem um aspecto rotulado ou cujo aspecto não faz parte da lista de aspectos mais relevantes\n","x = []\n","out = ['screen', 'price', 'keyboard', 'use', 'features', 'programs', 'Photobooth', 'processor', 'battery', 'software', 'hardware']\n","test1 = True\n","for i in range (len(labeled_df)):\n","    test1 = True\n","    if (labeled_df.aspects[i] == []):\n","        x.append(i)\n","    else:\n","        for f in labeled_df.aspects[i]:\n","            if f in out:\n","                test1 = False \n","        if test1:\n","            x.append(i)\n","print(\"Existem\", len(x), \"reviews selecionados para exclusão.\")"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Existem 2711 reviews selecionados para exclusão.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yyBbkXHseXwW","colab_type":"code","colab":{}},"source":["#x = []\n","#for i in range (len(labeled_df)):\n","#    test1 = True\n","#    if (labeled_df.aspects[i] == []):\n","#        x.append(i)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V3XRklbTeXwd","colab_type":"code","colab":{}},"source":["#out = ['screen', 'price', 'keyboard', 'use', 'features', 'programs', 'Photobooth',\n","# 'screen', 'processor', 'battery', 'software', 'hardware']\n","#\"scree\" not in out\n","#len(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SX1z2aGieXws","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b747dd7d-60b8-41c9-fad8-19a53aecee2f","executionInfo":{"status":"ok","timestamp":1575517620897,"user_tz":180,"elapsed":945,"user":{"displayName":"Thiago Batista","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCEVEmptKeAM5s4J7Gr7nNwWwrHzQd8tnIfJQ63Eg=s64","userId":"15252053828205320610"}}},"source":["# Remove todos os reviews que não possuem aspectos rotulados\n","labeled_df = labeled_df.drop(labeled_df.index[x])\n","print(\"O tamanho final do conjunto de treinamento é\", len(labeled_df))"],"execution_count":36,"outputs":[{"output_type":"stream","text":["O tamanho final do conjunto de treinamento é 337\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bduy0uqseXw_","colab_type":"code","colab":{}},"source":["#asp = labeled_df['aspects'].apply(lambda x: ' '.join(sorted([i for i in x])) if x != [] else '')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8o79RvLpeXxD","colab_type":"code","colab":{}},"source":["# Exibe o total de vezes que cada aspecto aparece nos reviews\n","list_asp = (labeled_df['aspects'].apply(lambda x: [i for i in x] if x != [] else ''))\n","Counter([i for j in list_asp for i in j]).most_common()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8V0WR8F4eXxM","colab_type":"code","colab":{}},"source":["#unique_aspect = []\n","#for i in labeled_df['aspects']:\n","#    for f in range (len(i)):\n","#        if (not(i[f] in unique_aspect)):    \n","#            unique_aspect.append(i[f])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9oJ6lsPUeXxY","colab_type":"code","colab":{}},"source":["# Exibe a nuvem de palavras\n","text = \" \".join(review for review in labeled_df['text'])\n","text = remove_stopwords(text)\n","wordcloud = WordCloud(background_color=\"white\").generate(text)\n","\n","fig = plt.figure(figsize=(20, 30))\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.title('Núvem de palavras da lista de reviews')\n","plt.axis(\"off\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eOtgsYFYRkwt","colab":{}},"source":["# Salva a lista de reviews anotada\n","labeled_df.to_pickle(\"annotated_reviews_df.pkl\")\n","labeled_df.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4fHJNzDKRoks","colab":{}},"source":["# Função para substituir os pronomes\n","def replace_pronouns(text):\n","    doc = nlp(text)\n","    return doc._.coref_resolved"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qNI0QqxNRtm0","colab":{}},"source":["# Carrega a lista de reviews anotada\n","annotated_reviews_df = pd.read_pickle(\"annotated_reviews_df.pkl\")\n","\n","# Cria uma nova coluna contendo o texto do review com a substituição dos pronomes\n","annotated_reviews_df[\"text_pro\"] = annotated_reviews_df.text.apply(lambda x: replace_pronouns(x))\n","\n","# Salva o dataframe com a nova coluna em um novo pickle\n","annotated_reviews_df.to_pickle(\"annotated_reviews_df2.pkl\")\n","\n","# Exibe os primeiros itens da nova coluna\n","annotated_reviews_df.text_pro.head(5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2trWN7MsSLn0","colab":{}},"source":["# Prepara os dados de entrada e de saída\n","mlb = MultiLabelBinarizer()\n","y = mlb.fit_transform(annotated_reviews_df.aspects)\n","X = annotated_reviews_df.text_pro\n","\n","# Separa os conjuntos de treinamento e de teste\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.25, random_state=0)\n","\n","# Salva o binarizador ajustado\n","filename = 'mlb.pkl'\n","pickle.dump(mlb, open(filename, 'wb'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ReapRxw6SMf6","colab":{}},"source":["# Cria um modelo baseado em multinomial naive bayes classification\n","text_clf = Pipeline([('vect', CountVectorizer(stop_words = \"english\",ngram_range=(1, 1))),\n","                     ('tfidf', TfidfTransformer(use_idf=False)),\n","                     ('clf', LabelPowerset(MultinomialNB(alpha=1e-1))),])\n","text_clf = text_clf.fit(X_train, y_train)\n","predicted = text_clf.predict(X_test)\n","\n","# Calcula a acurácia\n","np.mean(predicted == y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rk8tR23vSQVc","colab":{}},"source":["# Testa a performance de um modelo SVM\n","text_clf_svm = Pipeline([('vect', CountVectorizer()),\n","                         ('tfidf', TfidfTransformer()),\n","                         ('clf-svm', LabelPowerset(\n","                             SGDClassifier(loss='hinge', penalty='l2',\n","                                           alpha=1e-3, max_iter=6, random_state=42)))])\n","_ = text_clf_svm.fit(X_train, y_train)\n","predicted_svm = text_clf_svm.predict(X_test)\n","\n","#Calculate accuracy\n","np.mean(predicted_svm == y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"B_2YifKYSRP0","colab":{}},"source":["# Treina o modelo naive bayes com todo o dataset\n","text_clf = Pipeline([('vect', CountVectorizer(stop_words = \"english\",ngram_range=(1, 1))),\n","                     ('tfidf', TfidfTransformer(use_idf=False)),\n","                     ('clf', LabelPowerset(MultinomialNB(alpha=1e-1))),])\n","text_clf = text_clf.fit(X, y)\n","\n","# Salva o modelo para utilizar na fase de análise de sentimentos\n","filename = 'naive_model1.pkl'\n","pickle.dump(text_clf, open(filename, 'wb'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AQ1bXWvZeXzI","colab_type":"text"},"source":["## Análise de sentimentos"]},{"cell_type":"code","metadata":{"id":"-sLvp8OCeXzM","colab_type":"code","colab":{}},"source":["#-------------------------------------------------------------\n","# Carrega um word embedding treinado com o Google News dataset\n","#-------------------------------------------------------------\n","google_news_path = 'C:/Users/rpga0/Documents/GoogleNews-vectors-negative300.bin'\n","\n","# Load google news vecs in gensim\n","model = gensim.models.KeyedVectors.load_word2vec_format(google_news_path, binary=True)\n","\n","# Init blank english spacy nlp object\n","nlp = spacy.blank('en')\n","\n","# Loop through range of all indexes, get words associated with each index.\n","# The words in the keys list will correspond to the order of the google embed matrix\n","keys = []\n","for idx in range(3000000):\n","    keys.append(model.index2word[idx])\n","\n","# Set the vectors for our nlp object to the google news vectors\n","#nlp.vocab.vectors = spacy.vocab.Vectors(data=model.syn0, keys=keys)\n","\n","#>>> nlp.vocab.vectors.shape\n","#(3000000, 300)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wWV4wZz5eXzP","colab_type":"code","colab":{}},"source":["nlp.vocab.vectors = spacy.vocab.Vectors(data=model.syn0, keys=keys)\n","nlp.vocab.vectors.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qlqnq0VueXzV","colab_type":"code","colab":{}},"source":["word2vec = model\n","\n","# Testa a similaridade entre duas palavras\n","model.n_similarity(['windows'], [\"software\"])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gJ7kOPIZeXzk","colab_type":"code","colab":{}},"source":["#------------------------------------------------------------------------------------\n","# Verifica se existe similaridade entre uma palavra e algum item da lista de aspectos\n","#------------------------------------------------------------------------------------\n","def check_similarity(aspects, word):\n","    similarity = []\n","    for aspect in aspects:\n","        similarity.append(word2vec.n_similarity([aspect], [word]))\n","    \n","    # Define uma similaridade mínima de 0.30\n","    if max(similarity) > 0.30:\n","        # Retorna o aspecto que tem a maior similaridade com a palavra\n","        return aspects[np.argmax(similarity)]\n","    else:\n","        return None"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G5Uw86GBeXz1","colab_type":"code","colab":{}},"source":["#------------------------------------------------------------------------------------\n","# Relaciona termos aos aspectos e indica se o sentimento é positivo ou negativo\n","#------------------------------------------------------------------------------------\n","def assign_term_to_aspect(aspect_sent, terms_dict, sent_dict, pred):\n"," \n","     # Lista de principais aspectos\n","    aspects = ['screen', 'price', 'hardware', 'software', 'battery']\n","    \n","    for term in sent_dict:\n","        try:\n","            # Primeiro tenta fazer a relação utilizando word2vec\n","            if check_similarity(aspects, term.split()[-1]):\n","                terms_dict[check_similarity(aspects, term.split()[-1])][term] += sent_dict[term]\n","                if sent_dict[term] > 0:\n","                    aspect_sent[check_similarity(aspects, term.split()[-1])][\"pos\"] += sent_dict[term]\n","                else:\n","                    aspect_sent[check_similarity(aspects, term.split()[-1])][\"neg\"] += abs(sent_dict[term])\n","            # Agora tenta relacionar utilizando o modelo NB treinado\n","            elif (pred[0] == \"anecdotes/miscellaneous\"):\n","                continue\n","            elif (len(pred) == 1):\n","                terms_dict[pred[0]][term] += sent_dict[term]\n","                if sent_dict[term] > 0:\n","                    aspect_sent[pred[0]][\"pos\"] += sent_dict[term]\n","                else:\n","                    aspect_sent[pred[0]][\"neg\"] += abs(sent_dict[term])\n","            # Em último caso, classifica como miscelanious\n","            else:\n","                terms_dict[\"misc\"][term] += sent_dict[term]\n","                if sent_dict[term] > 0:\n","                    aspect_sent[\"misc\"][\"pos\"] += sent_dict[term]\n","                else:\n","                    aspect_sent[\"misc\"][\"neg\"] += abs(sent_dict[term])\n","        except:\n","            print(term, \"not in vocab\")\n","            continue\n","    return aspect_sent, terms_dict"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ovXpgZf_eXz7","colab_type":"code","colab":{}},"source":["nlseg = NewLineSegmenter()\n","nlp2 = spacy.load('en_core_web_lg')\n","nlp2.add_pipe(nlseg.set_sent_starts, name='sentence_segmenter', before='parser')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7wjtYlFueX0G","colab_type":"code","colab":{}},"source":["#------------------------------------------------------------------------------------\n","# Adiciona termos ao dicionário e qualifica o sentimento\n","#------------------------------------------------------------------------------------\n","def feature_sentiment(sentence):\n","    '''\n","    input: dictionary and sentence\n","    function: appends dictionary with new features if the feature did not exist previously,\n","              then updates sentiment to each of the new or existing features\n","    output: updated dictionary\n","    '''\n","    sentence = nlp(sentence)\n","    sent_dict = Counter()\n","    #sentence = nlp(sentence)\n","    debug = 0\n","\n","    for token in sentence:\n","       # print(token)\n","        #print(token.text,token.dep_, token.head, token.head.dep_)\n","\n","        # check if the word is an opinion word, then assign sentiment\n","        if token.text in opinion_words:\n","            #print(\"sim, \", token)\n","            sentiment = 1 if token.text in pos else -1\n","\n","            # if target is an adverb modifier (i.e. pretty, highly, etc.)\n","            # but happens to be an opinion word, ignore and pass\n","            if (token.dep_ == \"advmod\"):\n","                continue\n","            elif (token.dep_ == \"amod\"):\n","                sent_dict[token.head.text] += sentiment\n","            # for opinion words that are adjectives, adverbs, verbs...\n","            else:\n","                for child in token.children:\n","                    # if there's a adj modifier (i.e. very, pretty, etc.) add more weight to sentiment\n","                    # This could be better updated for modifiers that either positively or negatively emphasize\n","                    if ((child.dep_ == \"amod\") or (child.dep_ == \"advmod\")) and (child.text in opinion_words):\n","                        sentiment *= 1.5\n","                    # check for negation words and flip the sign of sentiment\n","                    if child.dep_ == \"neg\":\n","                        sentiment *= -1\n","\n","                for child in token.children:\n","                    # if verb, check if there's a direct object\n","                    if (token.pos_ == \"VERB\") & (child.dep_ == \"dobj\"):                        \n","                        sent_dict[child.text] += sentiment\n","                        # check for conjugates (a AND b), then add both to dictionary\n","                        subchildren = []\n","                        conj = 0\n","                        for subchild in child.children:\n","                            if subchild.text == \"and\":\n","                                conj=1\n","                            if (conj == 1) and (subchild.text != \"and\"):\n","                                subchildren.append(subchild.text)\n","                                conj = 0\n","                        for subchild in subchildren:\n","                            sent_dict[subchild] += sentiment\n","\n","                # check for negation\n","                for child in token.head.children:\n","                    noun = \"\"\n","                    if ((child.dep_ == \"amod\") or (child.dep_ == \"advmod\")) and (child.text in opinion_words):\n","                        sentiment *= 1.5\n","                    # check for negation words and flip the sign of sentiment\n","                    if (child.dep_ == \"neg\"): \n","                        sentiment *= -1\n","                \n","                # check for nouns\n","                for child in token.head.children:\n","                    noun = \"\"\n","                    if (child.pos_ == \"NOUN\") and (child.text not in sent_dict):\n","                        noun = child.text\n","                        # Check for compound nouns\n","                        for subchild in child.children:\n","                            if subchild.dep_ == \"compound\":\n","                                noun = subchild.text + \" \" + noun\n","                        sent_dict[noun] += sentiment\n","                    debug += 1\n","    return sent_dict\n","\n","#------------------------------------------------------------------------------------\n","# Classifica uma sentença em uma categoria e associa a um sentimento\n","#------------------------------------------------------------------------------------\n","def classify_and_sent(sentence, aspect_sent, terms_dict):\n","    # Classifica a sentença utilizando o classificador NB\n","    naive_model1 = pickle.load(open(\"naive_model1.pkl\", 'rb'))\n","    predicted = naive_model1.predict([sentence])\n","    pred = mlb.inverse_transform(predicted)\n","\n","    sent_dict = feature_sentiment(sentence)\n","    aspect_sent, terms_dict = assign_term_to_aspect(aspect_sent, terms_dict, sent_dict, pred[0])\n","\n","    return aspect_sent, terms_dict\n","\n","#------------------------------------------------------------------------------------\n","# Separa o review em uma lista de sentenças usando o spacy's sentence parser\n","#------------------------------------------------------------------------------------\n","def split_sentence(text):\n","    review = nlp(text)\n","    bag_sentence = []\n","    start = 0\n","\n","    for token in review:\n","        if token.sent_start:\n","            bag_sentence.append(review[start:(token.i-1)])\n","            start = token.i\n","        if token.i == len(review)-1:\n","            bag_sentence.append(review[start:(token.i+1)])\n","\n","    return bag_sentence\n","\n","#------------------------------------------------------------------------------------\n","# Remove caracteres especiais utilizando RegEx\n","#------------------------------------------------------------------------------------\n","def remove_special_char(sentence):\n","    return re.sub(r\"[^a-zA-Z0-9.',:;?]+\", ' ', sentence)\n","\n","#------------------------------------------------------------------------------------\n","# Faz a análise de sentimento em cada sentença de um review\n","#------------------------------------------------------------------------------------\n","def review_pipe(review, aspect_sent, terms_dict={'screen':Counter(), 'price':Counter(),'hardware':Counter(),'software':Counter(),'battery':Counter()}):\n","    review = replace_pronouns(review)\n","    sentences = split_sentence(review)\n","    for sentence in sentences:\n","        sentence = remove_special_char(str(sentence))\n","        aspect_sent, terms_dict = classify_and_sent(sentence.lower(), aspect_sent, terms_dict)\n","    return aspect_sent, terms_dict"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ckug8yLmeX0J","colab_type":"code","colab":{}},"source":["# Teste da análise de sentimento para uma sentença\n","sentence = \"I bought it from HSN because it was \\\"bundled\\\" with extra software, but as it turns out, that software just crashes it more often\"\n","feature_sentiment(sentence)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"spfeDS8peX0N","colab_type":"code","colab":{}},"source":["# Teste da análise de sentimento sobre um review\n","terms_dict={'screen':Counter(), 'price':Counter(), 'hardware':Counter(),\n","            'software':Counter(), 'battery':Counter()}\n","aspect_sent={'screen':Counter(), 'price':Counter(), 'hardware':Counter(),\n","            'software':Counter(),'battery':Counter()}\n","review = \"I bought it from HSN because it was \\\"bundled\\\" with extra Software, but as it turns out, that software just crashes it more often\"\n","review_pipe(review, aspect_sent, terms_dict)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UqFz3H4keX0R","colab_type":"code","colab":{}},"source":["# Faz a análise de sentimento em toda a base de teste\n","for txt in X_test:\n","    review = txt\n","    aspect_sent, terms_dict = review_pipe(review, aspect_sent, terms_dict)\n","    print(\"Lista de aspectos \", aspect_sent, \"/n\")\n","    print(\"Lista de termos\", terms_dict, \"\\n\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mIt3ek7feX0X","colab_type":"code","colab":{}},"source":["aspect_sent"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MpJz_HqmeX0a","colab_type":"code","colab":{}},"source":["terms_dict"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RM395kHXeX0d","colab_type":"code","colab":{}},"source":["displacy.render(nlp2(sentence), style='dep')"],"execution_count":0,"outputs":[]}]}